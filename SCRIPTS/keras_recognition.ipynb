{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial for image recognition using keras:\n",
    "\n",
    "https://www.sitepoint.com/keras-face-detection-recognition/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary libraries:\n",
    "\n",
    "pip install facenet-pytorch\n",
    "\n",
    "pip install numpy\n",
    "\n",
    "pip install pandas\n",
    "\n",
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's ensure that our image preprocessing was successful, generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate images found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize MTCNN (for face detection) and InceptionResnetV1 (for facial embeddings)\n",
    "mtcnn = MTCNN(keep_all=True)\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Function to generate facial encoding (embedding) for an image\n",
    "def get_face_encoding(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img_cropped = mtcnn(img)  # Detect faces in the image\n",
    "    if img_cropped is None:\n",
    "        return None  # No face detected, return None\n",
    "    img_embedding = model(img_cropped)  # Extract the facial embedding\n",
    "    return img_embedding.detach().numpy().flatten()  # Return as 1D array\n",
    "\n",
    "# Check similarity between two sample images\n",
    "image_path_1 = \"path_to_image_01.jpg\"\n",
    "image_path_2 = \"path_to_image_02.jpg\"\n",
    "\n",
    "encoding_1 = get_face_encoding(image_path_1)\n",
    "encoding_2 = get_face_encoding(image_path_2)\n",
    "\n",
    "if encoding_1 is not None and encoding_2 is not None:\n",
    "    similarity = cosine_similarity([encoding_1], [encoding_2])[0][0]\n",
    "    print(f\"Cosine similarity between {image_path_1} and {image_path_2}: {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"Face not detected in one or both images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This indicates that no two images in our dataset have the same embedding (facial encoding).\n",
    "\n",
    "## Let's choose two images that look very similar on which to test this. We will run a cosine similarity test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.pyenv_mirror/user/current/lib/python3.12/site-packages/facenet_pytorch/models/mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "/workspace/.pyenv_mirror/user/current/lib/python3.12/site-packages/facenet_pytorch/models/mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "/workspace/.pyenv_mirror/user/current/lib/python3.12/site-packages/facenet_pytorch/models/mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "/workspace/.pyenv_mirror/user/current/lib/python3.12/site-packages/facenet_pytorch/models/inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MTCNN (for face detection) and InceptionResnetV1 (for facial embeddings)\n",
    "mtcnn = MTCNN(keep_all=True)\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Function to generate facial encoding (embedding) for an image\n",
    "def get_face_encoding(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img_cropped = mtcnn(img)  # Detect faces in the image\n",
    "    if img_cropped is None:\n",
    "        return None  # No face detected, return None\n",
    "    img_embedding = model(img_cropped)  # Extract the facial embedding\n",
    "    return img_embedding.detach().numpy().flatten()  # Return as 1D array (512-dimensional)\n",
    "\n",
    "# Example usage\n",
    "image_path_1 = \"path_to_image_01.jpg\"\n",
    "image_path_2 = \"path_to_image_02.jpg\"\n",
    "\n",
    "encoding_1 = get_face_encoding(image_path_1)\n",
    "encoding_2 = get_face_encoding(image_path_2)\n",
    "\n",
    "# Check if the embeddings have the same dimensionality\n",
    "print(f\"Encoding 1 shape: {encoding_1.shape}\")\n",
    "print(f\"Encoding 2 shape: {encoding_2.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
