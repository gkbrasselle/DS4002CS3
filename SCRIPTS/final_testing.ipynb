{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Testing phase\n",
    "\n",
    "### Goal: Using Keras, implement _______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split # for splitting data into training / test sets\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # \n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "data_dir = \"/workspace/DS4002Project3/DATA/celebrities_all\" # set base directory\n",
    "image_size = (150, 150)  # Resize images to 150 x 150\n",
    "batch_size = 32 # batch size refers to the number of images that will be processed at a time before the model's parameters are reset. \n",
    "# we chose this number becuase it is relatively standard.\n",
    "epochs = 100 # we settled on 100 epochs for the testing phase\n",
    "\n",
    "# using ImageDataGenerator, load / preprocess the images\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "# the \"rescale\" parameter scales down the range of pixel sizes in the images to between 0 and 1. This helps with processing speed.\n",
    "# \"validation-split\" defines a 20/80 test/training split (0.2 = ratio of images placed in testing category)\n",
    "\n",
    "# Training data generator -- method to produce a set of images based on chosen characteristics\n",
    "train_generator = datagen.flow_from_directory( \n",
    "    data_dir, # specifies path to target directory (\"celebrities_all\")\n",
    "    target_size=image_size, # standardizes image size to 150 x 150, as specified earlier\n",
    "    batch_size=batch_size, # batch size is 32, as specified earlier\n",
    "    class_mode='categorical', # the model will be identifying celeb images, where each celeb is a different \"class\". Celeb is a categorical variable\n",
    "    subset='training' # the generator will pull the training images, which make up 80% of the celebrities_all images.\n",
    ")\n",
    "\n",
    "# Validation data generator -- repeating the steps used to define the train_generator. Parameters are the same, but the generator will pull from the 20% of images in the test (validation) set\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation' # pulling from the validation set\n",
    ")\n",
    "\n",
    "# Now, we will build a simple CNN model using Keras. \n",
    "# First, using the models.sequential class, a sequential model will be created to produce a stack of layers. Each layer will have an input\n",
    "# sensor and an output sensor. This will create a \"feed-forward\" neural network wherein each layer is directly connected to the one before it.\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 3. Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# 4. Evaluate the model and store accuracies\n",
    "accuracy_results = model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {accuracy_results[1]}\")\n",
    "\n",
    "# Create a DataFrame to store the accuracy of each image\n",
    "celebs_accuracy = pd.DataFrame({\n",
    "    \"image_path\": validation_generator.filenames,\n",
    "    \"accuracy\": [accuracy_results[1]] * len(validation_generator.filenames)\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file (optional)\n",
    "celebs_accuracy.to_csv(\"celebs_accuracy.csv\", index=False)\n",
    "\n",
    "print(\"Accuracy data saved to celebs_accuracy.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
